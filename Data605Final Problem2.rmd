---
title: "Data 605 Final Problem #2:"
author: "Al Haque"
date: '2022-12-11'
output: html_document
---

```{r}
## set the work directory to where the file is and then read it... 
library(tidyverse)
library(OpenImageR)
train <- read_csv("train.csv")
test <- read_csv("test.csv")
```

```{r}
## divide the training data by 255
## The labels tell what number is drawn by the image so the first label is an image of a 1 
label = train[,1]
training_data = train[,-1]/255

```

```{r}
## Each pixel is a 28 by 28 matrix
img1<- matrix(unlist(training_data[10,-1]),nrow=28,ncol=28,byrow=TRUE)
image(img1,useRaster=TRUE)

## Maybe I should rotate it?? ## Fixed it
r <- rotateFixed(img1, 90)
image(r,useRaster=TRUE)
```

```{r,fig.width = 4}
## printing the first ten images:
for (i in 1:10){
  img1<- matrix(unlist(training_data[i,-1]),nrow=28,ncol=28,byrow=TRUE)
  rot <- rotateFixed(img1, 90)
  image(rot,useRaster=TRUE)
  
}
```
  
### 4.What is the frequency distribution of the numbers in the dataset?


```{r}
## Group the data by label and then count the number of occurences... 
Freq_Dist <- train %>%
  group_by(label) %>%
  summarise(Occurences = n())
Freq_Dist

```

There is a balanced number of labels within the training dataset.. 


```{r}
## The visualization 
ggplot(data=Freq_Dist, aes(x=label, y=Occurences)) +
  geom_bar(stat="identity", fill="red") +
  geom_text(aes(label=Occurences))
```



### 5. For each number, provide the mean pixel intensity. What does this tell you?
    The mean pixel intensity is the average primary information in the pixel,or according to adobe forums it is the average brightness so the average brightness is around 0.02 which is closer to 0 we can see that the average pixel intensity is closer to the darker spectrum than the lighter spectrum since we have values ranging between 0 to 1.. 

```{r}
train %>%
  group_by(label) %>%
  summarise(Avg = (n()/784)/255)
```

###6.Reduce the data by using principal components that account for 95% of the variance.
For 95% of the variance i got 154 components and for 100% of the variance i got 704 components..

```{r}
### Use the training data without the labels::
train_pca <- prcomp(training_data)

## I had some help with this and we just compute the variance by calculating the standard deviation and finding the variance:
## thanks statology
var <- cumsum(train_pca$sdev^2) / sum(train_pca$sdev^2)
```

```{r}
## Variance that accounts for 95% of the variance:
which.max(var >= 0.95)
```

```{r}
which.max(var >= 1.00)
```

###7.Plot the first 10 images generated by PCA. They will appear to be noise. Why?
  
  Pca only reduces the dimensionalty of the dataset not the noise of the dataset.This looks really blurry so i am not sure if it is even correct..  

```{r,fig.width = 4}
## The principal components are stored in pca$rotation arguments where we have to iterate thorugh the columns it appears:
## this took a while sheesh
for (i in 1:10)
{
 image(matrix(train_pca$rotation[,i], nrow=28, ncol=28),axes=FALSE)
}


```

### 8. Now, select only those images that have labels that are 8â€™s. Re-run PCA that accounts for all of the variance (100%). Plot the first 10 images

```{r}
## select all labels with 8
eight_label <- train %>%
  group_by(label) %>%
  filter(label == 8)
```


```{r}
## take out the label column:

eight_label <- eight_label[,-1]
```

```{r}
## run the pca
eight_pca <- prcomp(eight_label)

## calculate the variance:
eight_var <- cumsum(eight_pca$sdev^2)/ sum(eight_pca$sdev^2)
```

```{r}
## find the variance that accounts for all 100: we have 537 components that accounts for 100 of the variance: 
which.max(eight_var == 1.0)
```

```{r,fig.width = 4}
## The principal components are stored in pca$rotation arguments where we have to iterate thorugh the columns it appears:
## I can see all eights nice.. 
for (i in 1:10)
{
 image(matrix(eight_pca$rotation[,i], nrow=28, ncol=28),axes=FALSE)
}
```

### 9. Build a multinomial model: 

```{r}
## Let us split the training dataset into a train and test datasets: 60% and 40% 
Train <- sample_frac(train,0.6)
sample_train <- as.numeric(rownames(Train)) 
Test <- train[-sample_train,]
```


```{r}
## creating the model: 
require(nnet)
model.fit <- multinom(label~.,data=Train,MaxNWts = 42000)
```

```{r}
## using the model to predict the testing dataset:
predictions <- predict(model.fit,Test,"class")

```

```{r}
confusion_matrix <- as.matrix(table("label" = Test$label,"predictions" = predictions))
confusion_matrix
```

```{r}
# Calculating accuracy - sum of diagonal elements divided by total obs I got a 91% accuracy.. 
round((sum(diag(confusion_matrix))/sum(confusion_matrix))*100,2)
```

#### Sources: (I had a lot of help especially with PCA)
<https://www.statology.org/principal-components-analysis-in-r/>
<https://datasciencebeginners.com/2018/12/20/multinomial-logistic-regression-using-r/>
<https://www.projectpro.io/recipes/split-data-into-train-set-and-test-set-r>
<https://www.datacamp.com/tutorial/pca-analysis-r> 